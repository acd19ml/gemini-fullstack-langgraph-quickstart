{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "_ = load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# State\n",
    "from langgraph.graph import add_messages\n",
    "from typing_extensions import Annotated\n",
    "from typing import TypedDict\n",
    "import operator\n",
    "\n",
    "class OverallState(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "    search_query: Annotated[list, operator.add]\n",
    "    web_research_result: Annotated[list, operator.add]\n",
    "    sources_gathered: Annotated[list, operator.add]\n",
    "    initial_search_query_count: int\n",
    "    max_research_loops: int\n",
    "    research_loop_count: int\n",
    "    reasoning_model: str\n",
    "\n",
    "class Query(TypedDict):\n",
    "    query: str\n",
    "    rationale: str\n",
    "\n",
    "\n",
    "class QueryGenerationState(TypedDict):\n",
    "    query_list: list[Query]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "\n",
    "import os\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Any, Optional\n",
    "\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "\n",
    "\n",
    "class Configuration(BaseModel):\n",
    "    \"\"\"The configuration for the agent.\"\"\"\n",
    "\n",
    "    query_generator_model: str = Field(\n",
    "        default=\"gemini-2.0-flash\",\n",
    "        metadata={\n",
    "            \"description\": \"The name of the language model to use for the agent's query generation.\"\n",
    "        },\n",
    "    )\n",
    "\n",
    "    reflection_model: str = Field(\n",
    "        default=\"gemini-2.5-flash-preview-04-17\",\n",
    "        metadata={\n",
    "            \"description\": \"The name of the language model to use for the agent's reflection.\"\n",
    "        },\n",
    "    )\n",
    "\n",
    "    answer_model: str = Field(\n",
    "        default=\"gemini-2.5-pro-preview-05-06\",\n",
    "        metadata={\n",
    "            \"description\": \"The name of the language model to use for the agent's answer.\"\n",
    "        },\n",
    "    )\n",
    "\n",
    "    number_of_initial_queries: int = Field(\n",
    "        default=3,\n",
    "        metadata={\"description\": \"The number of initial search queries to generate.\"},\n",
    "    )\n",
    "\n",
    "    max_research_loops: int = Field(\n",
    "        default=2,\n",
    "        metadata={\"description\": \"The maximum number of research loops to perform.\"},\n",
    "    )\n",
    "\n",
    "    @classmethod\n",
    "    def from_runnable_config(\n",
    "        cls, config: Optional[RunnableConfig] = None\n",
    "    ) -> \"Configuration\":\n",
    "        \"\"\"Create a Configuration instance from a RunnableConfig.\"\"\"\n",
    "        configurable = (\n",
    "            config[\"configurable\"] if config and \"configurable\" in config else {}\n",
    "        )\n",
    "\n",
    "        # Get raw values from environment or config\n",
    "        raw_values: dict[str, Any] = {\n",
    "            name: os.environ.get(name.upper(), configurable.get(name))\n",
    "            for name in cls.model_fields.keys()\n",
    "        }\n",
    "\n",
    "        # Filter out None values\n",
    "        values = {k: v for k, v in raw_values.items() if v is not None}\n",
    "\n",
    "        return cls(**values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tools and Schemas\n",
    "\n",
    "from typing import List\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "class SearchQueryList(BaseModel):\n",
    "    query: List[str] = Field(\n",
    "        description=\"A list of search queries to be used for web research.\"\n",
    "    )\n",
    "    rationale: str = Field(\n",
    "        description=\"A brief explanation of why these queries are relevant to the research topic.\"\n",
    "    )\n",
    "\n",
    "\n",
    "class Reflection(BaseModel):\n",
    "    is_sufficient: bool = Field(\n",
    "        description=\"Whether the provided summaries are sufficient to answer the user's question.\"\n",
    "    )\n",
    "    knowledge_gap: str = Field(\n",
    "        description=\"A description of what information is missing or needs clarification.\"\n",
    "    )\n",
    "    follow_up_queries: List[str] = Field(\n",
    "        description=\"A list of follow-up queries to address the knowledge gap.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompts\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "# Get current date in a readable format\n",
    "def get_current_date():\n",
    "    return datetime.now().strftime(\"%B %d, %Y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompts\n",
    "\n",
    "query_writer_instructions = \"\"\"Your goal is to generate sophisticated and diverse web search queries. These queries are intended for an advanced automated web research tool capable of analyzing complex results, following links, and synthesizing information.\n",
    "\n",
    "Instructions:\n",
    "- Always prefer a single search query, only add another query if the original question requests multiple aspects or elements and one query is not enough.\n",
    "- Each query should focus on one specific aspect of the original question.\n",
    "- Don't produce more than {number_queries} queries.\n",
    "- Queries should be diverse, if the topic is broad, generate more than 1 query.\n",
    "- Don't generate multiple similar queries, 1 is enough.\n",
    "- Query should ensure that the most current information is gathered. The current date is {current_date}.\n",
    "\n",
    "Format: \n",
    "- Format your response as a JSON object with ALL three of these exact keys:\n",
    "   - \"rationale\": Brief explanation of why these queries are relevant\n",
    "   - \"query\": A list of search queries\n",
    "\n",
    "Example:\n",
    "\n",
    "Topic: What revenue grew more last year apple stock or the number of people buying an iphone\n",
    "```json\n",
    "{{\n",
    "    \"rationale\": \"To answer this comparative growth question accurately, we need specific data points on Apple's stock performance and iPhone sales metrics. These queries target the precise financial information needed: company revenue trends, product-specific unit sales figures, and stock price movement over the same fiscal period for direct comparison.\",\n",
    "    \"query\": [\"Apple total revenue growth fiscal year 2024\", \"iPhone unit sales growth fiscal year 2024\", \"Apple stock price growth fiscal year 2024\"],\n",
    "}}\n",
    "```\n",
    "\n",
    "Context: {research_topic}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utils\n",
    "\n",
    "from typing import List\n",
    "from langchain_core.messages import AnyMessage, AIMessage, HumanMessage\n",
    "\n",
    "\n",
    "def get_research_topic(messages: List[AnyMessage]) -> str:\n",
    "    \"\"\"\n",
    "    Get the research topic from the messages.\n",
    "    \"\"\"\n",
    "    # check if request has a history and combine the messages into a single string\n",
    "    if len(messages) == 1:\n",
    "        research_topic = messages[-1].content\n",
    "    else:\n",
    "        research_topic = \"\"\n",
    "        for message in messages:\n",
    "            if isinstance(message, HumanMessage):\n",
    "                research_topic += f\"User: {message.content}\\n\"\n",
    "            elif isinstance(message, AIMessage):\n",
    "                research_topic += f\"Assistant: {message.content}\\n\"\n",
    "    return research_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Graph\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "def generate_query(state: OverallState, config: RunnableConfig) -> QueryGenerationState:\n",
    "    \"\"\"LangGraph node that generates a search queries based on the User's question.\n",
    "\n",
    "    Uses Gemini 2.0 Flash to create an optimized search query for web research based on\n",
    "    the User's question.\n",
    "\n",
    "    Args:\n",
    "        state: Current graph state containing the User's question\n",
    "        config: Configuration for the runnable, including LLM provider settings\n",
    "\n",
    "    Returns:\n",
    "        Dictionary with state update, including search_query key containing the generated query\n",
    "    \"\"\"\n",
    "    configurable = Configuration.from_runnable_config(config)\n",
    "\n",
    "    # check for custom initial search query count\n",
    "    if state.get(\"initial_search_query_count\") is None:\n",
    "        state[\"initial_search_query_count\"] = configurable.number_of_initial_queries\n",
    "\n",
    "    # init Gemini 2.0 Flash\n",
    "    llm = ChatGoogleGenerativeAI(\n",
    "        model=configurable.query_generator_model,\n",
    "        temperature=1.0,\n",
    "        max_retries=2,\n",
    "        api_key=os.getenv(\"GEMINI_API_KEY\"),\n",
    "    )\n",
    "    structured_llm = llm.with_structured_output(SearchQueryList)\n",
    "\n",
    "    # Format the prompt\n",
    "    current_date = get_current_date()\n",
    "    formatted_prompt = query_writer_instructions.format(\n",
    "        current_date=current_date,\n",
    "        research_topic=get_research_topic(state[\"messages\"]),\n",
    "        number_queries=state[\"initial_search_query_count\"],\n",
    "    )\n",
    "    # Generate the search queries\n",
    "    result = structured_llm.invoke(formatted_prompt)\n",
    "    print (formatted_prompt)\n",
    "    return {\"query_list\": result.query}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your goal is to generate sophisticated and diverse web search queries. These queries are intended for an advanced automated web research tool capable of analyzing complex results, following links, and synthesizing information.\n",
      "\n",
      "Instructions:\n",
      "- Always prefer a single search query, only add another query if the original question requests multiple aspects or elements and one query is not enough.\n",
      "- Each query should focus on one specific aspect of the original question.\n",
      "- Don't produce more than 3 queries.\n",
      "- Queries should be diverse, if the topic is broad, generate more than 1 query.\n",
      "- Don't generate multiple similar queries, 1 is enough.\n",
      "- Query should ensure that the most current information is gathered. The current date is July 04, 2025.\n",
      "\n",
      "Format: \n",
      "- Format your response as a JSON object with ALL three of these exact keys:\n",
      "   - \"rationale\": Brief explanation of why these queries are relevant\n",
      "   - \"query\": A list of search queries\n",
      "\n",
      "Example:\n",
      "\n",
      "Topic: What revenue grew more last year apple stock or the number of people buying an iphone\n",
      "```json\n",
      "{\n",
      "    \"rationale\": \"To answer this comparative growth question accurately, we need specific data points on Apple's stock performance and iPhone sales metrics. These queries target the precise financial information needed: company revenue trends, product-specific unit sales figures, and stock price movement over the same fiscal period for direct comparison.\",\n",
      "    \"query\": [\"Apple total revenue growth fiscal year 2024\", \"iPhone unit sales growth fiscal year 2024\", \"Apple stock price growth fiscal year 2024\"],\n",
      "}\n",
      "```\n",
      "\n",
      "Context: 2024年AI领域最重要的研究进展有哪些？\n",
      "{'query_list': ['key AI research breakthroughs 2024', 'top AI advancements 2024', 'significant AI research papers published 2024']}\n"
     ]
    }
   ],
   "source": [
    "# generate_query 调用测试\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "\n",
    "# 构造 state\n",
    "state = {\n",
    "    \"messages\": [HumanMessage(content=\"2024年AI领域最重要的研究进展有哪些？\")],\n",
    "    \"search_query\": [],\n",
    "    \"web_research_result\": [],\n",
    "    \"sources_gathered\": [],\n",
    "    \"initial_search_query_count\": 3,\n",
    "    \"max_research_loops\": 2,\n",
    "    \"research_loop_count\": 0,\n",
    "    \"reasoning_model\": \"gemini-2.0-flash\"\n",
    "}\n",
    "\n",
    "# 构造 config\n",
    "config = RunnableConfig({})\n",
    "\n",
    "# 调用 generate_query\n",
    "result = generate_query(state, config)\n",
    "\n",
    "# 打印结果\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (.venv-gemini)",
   "language": "python",
   "name": "venv-gemini"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
