{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "_ = load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# State\n",
    "from typing import TypedDict\n",
    "from langgraph.graph import add_messages\n",
    "from typing_extensions import Annotated\n",
    "\n",
    "\n",
    "import operator\n",
    "\n",
    "class OverallState(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "    search_query: Annotated[list, operator.add]\n",
    "    web_research_result: Annotated[list, operator.add]\n",
    "    sources_gathered: Annotated[list, operator.add]\n",
    "    initial_search_query_count: int\n",
    "    max_research_loops: int\n",
    "    research_loop_count: int\n",
    "    reasoning_model: str\n",
    "\n",
    "class ReflectionState(TypedDict):\n",
    "    is_sufficient: bool\n",
    "    knowledge_gap: str\n",
    "    follow_up_queries: Annotated[list, operator.add]\n",
    "    research_loop_count: int\n",
    "    number_of_ran_queries: int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "import os\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Any, Optional\n",
    "\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "\n",
    "\n",
    "class Configuration(BaseModel):\n",
    "    \"\"\"The configuration for the agent.\"\"\"\n",
    "\n",
    "    query_generator_model: str = Field(\n",
    "        default=\"gemini-2.0-flash\",\n",
    "        metadata={\n",
    "            \"description\": \"The name of the language model to use for the agent's query generation.\"\n",
    "        },\n",
    "    )\n",
    "\n",
    "    reflection_model: str = Field(\n",
    "        default=\"gemini-2.5-flash-preview-04-17\",\n",
    "        metadata={\n",
    "            \"description\": \"The name of the language model to use for the agent's reflection.\"\n",
    "        },\n",
    "    )\n",
    "\n",
    "    answer_model: str = Field(\n",
    "        default=\"gemini-2.5-pro-preview-05-06\",\n",
    "        metadata={\n",
    "            \"description\": \"The name of the language model to use for the agent's answer.\"\n",
    "        },\n",
    "    )\n",
    "\n",
    "    number_of_initial_queries: int = Field(\n",
    "        default=3,\n",
    "        metadata={\"description\": \"The number of initial search queries to generate.\"},\n",
    "    )\n",
    "\n",
    "    max_research_loops: int = Field(\n",
    "        default=2,\n",
    "        metadata={\"description\": \"The maximum number of research loops to perform.\"},\n",
    "    )\n",
    "\n",
    "    @classmethod\n",
    "    def from_runnable_config(\n",
    "        cls, config: Optional[RunnableConfig] = None\n",
    "    ) -> \"Configuration\":\n",
    "        \"\"\"Create a Configuration instance from a RunnableConfig.\"\"\"\n",
    "        configurable = (\n",
    "            config[\"configurable\"] if config and \"configurable\" in config else {}\n",
    "        )\n",
    "\n",
    "        # Get raw values from environment or config\n",
    "        raw_values: dict[str, Any] = {\n",
    "            name: os.environ.get(name.upper(), configurable.get(name))\n",
    "            for name in cls.model_fields.keys()\n",
    "        }\n",
    "\n",
    "        # Filter out None values\n",
    "        values = {k: v for k, v in raw_values.items() if v is not None}\n",
    "\n",
    "        return cls(**values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "\n",
    "# Get current date in a readable format\n",
    "def get_current_date():\n",
    "    return datetime.now().strftime(\"%B %d, %Y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "reflection_instructions = \"\"\"You are an expert research assistant analyzing summaries about \"{research_topic}\".\n",
    "\n",
    "Instructions:\n",
    "- Identify knowledge gaps or areas that need deeper exploration and generate a follow-up query. (1 or multiple).\n",
    "- If provided summaries are sufficient to answer the user's question, don't generate a follow-up query.\n",
    "- If there is a knowledge gap, generate a follow-up query that would help expand your understanding.\n",
    "- Focus on technical details, implementation specifics, or emerging trends that weren't fully covered.\n",
    "\n",
    "Requirements:\n",
    "- Ensure the follow-up query is self-contained and includes necessary context for web search.\n",
    "\n",
    "Output Format:\n",
    "- Format your response as a JSON object with these exact keys:\n",
    "   - \"is_sufficient\": true or false\n",
    "   - \"knowledge_gap\": Describe what information is missing or needs clarification\n",
    "   - \"follow_up_queries\": Write a specific question to address this gap\n",
    "\n",
    "Example:\n",
    "```json\n",
    "{{\n",
    "    \"is_sufficient\": true, // or false\n",
    "    \"knowledge_gap\": \"The summary lacks information about performance metrics and benchmarks\", // \"\" if is_sufficient is true\n",
    "    \"follow_up_queries\": [\"What are typical performance benchmarks and metrics used to evaluate [specific technology]?\"] // [] if is_sufficient is true\n",
    "}}\n",
    "```\n",
    "\n",
    "Reflect carefully on the Summaries to identify knowledge gaps and produce a follow-up query. Then, produce your output following this JSON format:\n",
    "\n",
    "Summaries:\n",
    "{summaries}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from langchain_core.messages import AnyMessage, AIMessage, HumanMessage\n",
    "def get_research_topic(messages: List[AnyMessage]) -> str:\n",
    "    \"\"\"\n",
    "    Get the research topic from the messages.\n",
    "    \"\"\"\n",
    "    # check if request has a history and combine the messages into a single string\n",
    "    if len(messages) == 1:\n",
    "        research_topic = messages[-1].content\n",
    "    else:\n",
    "        research_topic = \"\"\n",
    "        for message in messages:\n",
    "            if isinstance(message, HumanMessage):\n",
    "                research_topic += f\"User: {message.content}\\n\"\n",
    "            elif isinstance(message, AIMessage):\n",
    "                research_topic += f\"Assistant: {message.content}\\n\"\n",
    "    return research_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Reflection(BaseModel):\n",
    "    is_sufficient: bool = Field(\n",
    "        description=\"Whether the provided summaries are sufficient to answer the user's question.\"\n",
    "    )\n",
    "    knowledge_gap: str = Field(\n",
    "        description=\"A description of what information is missing or needs clarification.\"\n",
    "    )\n",
    "    follow_up_queries: List[str] = Field(\n",
    "        description=\"A list of follow-up queries to address the knowledge gap.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableConfig\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "def reflection(state: OverallState, config: RunnableConfig) -> ReflectionState:\n",
    "    \"\"\"LangGraph node that identifies knowledge gaps and generates potential follow-up queries.\n",
    "\n",
    "    Analyzes the current summary to identify areas for further research and generates\n",
    "    potential follow-up queries. Uses structured output to extract\n",
    "    the follow-up query in JSON format.\n",
    "\n",
    "    Args:\n",
    "        state: Current graph state containing the running summary and research topic\n",
    "        config: Configuration for the runnable, including LLM provider settings\n",
    "\n",
    "    Returns:\n",
    "        Dictionary with state update, including search_query key containing the generated follow-up query\n",
    "    \"\"\"\n",
    "    configurable = Configuration.from_runnable_config(config)\n",
    "    # Increment the research loop count and get the reasoning model\n",
    "    state[\"research_loop_count\"] = state.get(\"research_loop_count\", 0) + 1\n",
    "    reasoning_model = state.get(\"reasoning_model\") or configurable.reasoning_model\n",
    "\n",
    "    # Format the prompt\n",
    "    current_date = get_current_date()\n",
    "    formatted_prompt = reflection_instructions.format(\n",
    "        current_date=current_date,\n",
    "        research_topic=get_research_topic(state[\"messages\"]),\n",
    "        summaries=\"\\n\\n---\\n\\n\".join(state[\"web_research_result\"]),\n",
    "    )\n",
    "    # init Reasoning Model\n",
    "    llm = ChatGoogleGenerativeAI(\n",
    "        model=reasoning_model,\n",
    "        temperature=1.0,\n",
    "        max_retries=2,\n",
    "        api_key=os.getenv(\"GEMINI_API_KEY\"),\n",
    "    )\n",
    "    result = llm.with_structured_output(Reflection).invoke(formatted_prompt)\n",
    "\n",
    "    return {\n",
    "        \"is_sufficient\": result.is_sufficient,\n",
    "        \"knowledge_gap\": result.knowledge_gap,\n",
    "        \"follow_up_queries\": result.follow_up_queries,\n",
    "        \"research_loop_count\": state[\"research_loop_count\"],\n",
    "        \"number_of_ran_queries\": len(state[\"search_query\"]),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'is_sufficient': False, 'knowledge_gap': 'The summary provides a high-level overview of model releases and application areas but lacks specific technical details, the nature of the advancements, and their potential impact in 2024. It mentions GPT-4o, Gemini 2.0, LLaMA 3.1 and progress in science, healthcare, and robotics, but does not explain what these advancements entail or their significance beyond their existence. More detailed information about the specific research breakthroughs, underlying technologies, and their practical or theoretical implications is missing ', 'follow_up_queries': ['What are the most significant research advancements in AI in 2024, including technical details and their potential impact?'], 'research_loop_count': 1, 'number_of_ran_queries': 1}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "\n",
    "state = {\n",
    "    \"messages\": [HumanMessage(content=\"2024年AI领域最重要的研究进展有哪些？\")],\n",
    "    \"search_query\": [\"major AI research breakthroughs 2024\"],\n",
    "    \"web_research_result\": [\n",
    "        \"2024年，AI领域取得了多项重大突破，包括OpenAI发布GPT-4o、Google发布Gemini 2.0、Meta发布LLaMA 3.1等。AI在科学、医疗、机器人等领域也有显著进展。\"\n",
    "    ],\n",
    "    \"sources_gathered\": [],\n",
    "    \"initial_search_query_count\": 3,\n",
    "    \"max_research_loops\": 2,\n",
    "    \"research_loop_count\": 0,\n",
    "    \"reasoning_model\": \"gemini-2.5-flash-preview-04-17\"\n",
    "}\n",
    "\n",
    "config = RunnableConfig({})\n",
    "\n",
    "result = reflection(state, config)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (.venv-gemini)",
   "language": "python",
   "name": "venv-gemini"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
