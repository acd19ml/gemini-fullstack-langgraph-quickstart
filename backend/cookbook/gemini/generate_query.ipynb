{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86ad5a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "_ = load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4da8d5ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class Query(BaseModel):\n",
    "    query: str = Field(..., description=\"The query to be executed\")\n",
    "    rationale: str = Field(..., description=\"The rationale for the query\")\n",
    "\n",
    "class QueryGenerationState(BaseModel):\n",
    "    query_list: list[Query] = Field(\n",
    "        default_factory=list,\n",
    "        description=\"The list of queries to be executed\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2fec4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "from typing import List, Optional\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "class StepType(str, Enum):\n",
    "    RESEARCH = \"research\"\n",
    "    PROCESSING = \"processing\"\n",
    "\n",
    "\n",
    "class Step(BaseModel):\n",
    "    need_search: bool = Field(..., description=\"Must be explicitly set for each step\")\n",
    "    title: str\n",
    "    description: str = Field(..., description=\"Specify exactly what data to collect\")\n",
    "    step_type: StepType = Field(..., description=\"Indicates the nature of the step\")\n",
    "    query_list: Optional[list[Query]] = Field(\n",
    "        default=None, description=\"The search query for the step\"\n",
    "    )\n",
    "    execution_res: Optional[str] = Field(\n",
    "        default=None, description=\"The Step execution result\"\n",
    "    )\n",
    "\n",
    "class Plan(BaseModel):\n",
    "    locale: str = Field(\n",
    "        ..., description=\"e.g. 'en-US' or 'zh-CN', based on the user's language\"\n",
    "    )\n",
    "    has_enough_context: bool\n",
    "    thought: str\n",
    "    title: str\n",
    "    steps: List[Step] = Field(\n",
    "        default_factory=list,\n",
    "        description=\"Research & Processing steps to get more context\",\n",
    "    )\n",
    "\n",
    "    class Config:\n",
    "        json_schema_extra = {\n",
    "            \"examples\": [\n",
    "                {\n",
    "                    \"has_enough_context\": False,\n",
    "                    \"thought\": (\n",
    "                        \"To understand the current market trends in AI, we need to gather comprehensive information.\"\n",
    "                    ),\n",
    "                    \"title\": \"AI Market Research Plan\",\n",
    "                    \"steps\": [\n",
    "                        {\n",
    "                            \"need_search\": True,\n",
    "                            \"title\": \"Current AI Market Analysis\",\n",
    "                            \"description\": (\n",
    "                                \"Collect data on market size, growth rates, major players, and investment trends in AI sector.\"\n",
    "                            ),\n",
    "                            \"step_type\": \"research\",\n",
    "                        }\n",
    "                    ],\n",
    "                }\n",
    "            ]\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d4ce9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import MessagesState\n",
    "\n",
    "class State(MessagesState):\n",
    "    \"\"\"State for the agent system, extends MessagesState with next field.\"\"\"\n",
    "\n",
    "    # Runtime Variables\n",
    "    locale: str = \"en-US\"\n",
    "    research_topic: str = \"\"\n",
    "    model_selection: str = None\n",
    "    observations: list[str] = []\n",
    "    plan_iterations: int = 0\n",
    "    current_plan: Plan | str = None\n",
    "    auto_accepted_plan: bool = False\n",
    "\n",
    "    # generate query\n",
    "    initial_search_query_count: int = 3\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "90494fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "\n",
    "# Define available LLM types\n",
    "LLMType = Literal[\"gemini-2.5-flash\", \"gemini-2.5-pro\", \"claude-3.5-sonnet\", \"gpt-4o-mini\", \"deepseek-chat\", \"doubao-1.5-pro-32k\"]\n",
    "\n",
    "# Define agent-LLM mapping\n",
    "AGENT_LLM_MAP: dict[str, LLMType] = {\n",
    "    \"coordinator\": \"gemini-2.5-flash\",\n",
    "    \"planner\": \"gemini-2.5-flash\",\n",
    "    \"researcher\": \"gpt-4o-mini\",\n",
    "    \"coder\": \"gpt-4o-mini\",\n",
    "    \"reporter\": \"gemini-2.5-flash\",\n",
    "    \"podcast_script_writer\": \"doubao-1.5-pro-32k\",\n",
    "    \"ppt_composer\": \"doubao-1.5-pro-32k\",\n",
    "    \"prose_writer\": \"doubao-1.5-pro-32k\",\n",
    "    \"prompt_enhancer\": \"doubao-1.5-pro-32k\",\n",
    "    \"generate_query\": \"gpt-4o-mini\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "51b42b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "from typing import Dict, Any\n",
    "\n",
    "def replace_env_vars(value: str) -> str:\n",
    "    \"\"\"Replace environment variables in string values.\"\"\"\n",
    "    if not isinstance(value, str):\n",
    "        return value\n",
    "    if value.startswith(\"$\"):\n",
    "        env_var = value[1:]\n",
    "        return os.getenv(env_var, env_var)\n",
    "    return value\n",
    "\n",
    "def process_dict(config: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"Recursively process dictionary to replace environment variables.\"\"\"\n",
    "    if not config:\n",
    "        return {}\n",
    "    result = {}\n",
    "    for key, value in config.items():\n",
    "        if isinstance(value, dict):\n",
    "            result[key] = process_dict(value)\n",
    "        elif isinstance(value, str):\n",
    "            result[key] = replace_env_vars(value)\n",
    "        else:\n",
    "            result[key] = value\n",
    "    return result\n",
    "\n",
    "_config_cache: Dict[str, Dict[str, Any]] = {}\n",
    "\n",
    "def load_yaml_config(file_path: str) -> Dict[str, Any]:\n",
    "    \"\"\"Load and process YAML configuration file.\"\"\"\n",
    "    # 如果文件不存在，返回{}\n",
    "    if not os.path.exists(file_path):\n",
    "        return {}\n",
    "\n",
    "    # 检查缓存中是否已存在配置\n",
    "    if file_path in _config_cache:\n",
    "        return _config_cache[file_path]\n",
    "\n",
    "    # 如果缓存中不存在，则加载并处理配置\n",
    "    with open(file_path, \"r\") as f:\n",
    "        config = yaml.safe_load(f)\n",
    "    processed_config = process_dict(config)\n",
    "\n",
    "    # 将处理后的配置存入缓存\n",
    "    _config_cache[file_path] = processed_config\n",
    "    return processed_config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "492afcbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "# Cache for LLM instances\n",
    "_llm_cache: dict[LLMType, ChatOpenAI] = {}\n",
    "\n",
    "def _get_config_file_path() -> str:\n",
    "    \"\"\"Get the path to the configuration file.\"\"\"\n",
    "    return str((Path(os.getcwd()) / \"conf.yaml\").resolve())\n",
    "\n",
    "def _get_llm_type_config_keys() -> dict[str, str]:\n",
    "    \"\"\"Get mapping of LLM types to their configuration keys.\"\"\"\n",
    "    return {\n",
    "        \"gemini-2.5-flash\": \"GEMINI_2_5_FLASH\",\n",
    "        \"gemini-2.5-pro\": \"GEMINI_2_5_PRO\",\n",
    "        \"gpt-4o-mini\": \"GPT_4O_MINI\",\n",
    "        \"doubao-1.5-pro-32k\": \"DOUBAO_1_5_PRO_32K\",\n",
    "        \"claude-3.5-sonnet\": \"CLAUDE_3_5_SONNET\",\n",
    "        \"deepseek-chat\": \"DEEPSEEK_CHAT\",\n",
    "    }\n",
    "\n",
    "def _get_env_llm_conf(llm_type: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Get LLM configuration from environment variables.\n",
    "    Environment variables should follow the format: {LLM_TYPE}__{KEY}\n",
    "    e.g., BASIC_MODEL__api_key, BASIC_MODEL__base_url\n",
    "    \"\"\"\n",
    "    prefix = f\"{llm_type.upper()}_MODEL__\"\n",
    "    conf = {}\n",
    "    for key, value in os.environ.items():\n",
    "        if key.startswith(prefix):\n",
    "            conf_key = key[len(prefix) :].lower()\n",
    "            conf[conf_key] = value\n",
    "    return conf\n",
    "\n",
    "def _create_llm_use_conf(\n",
    "    llm_type: LLMType, conf: Dict[str, Any]\n",
    ") -> ChatOpenAI | ChatGoogleGenerativeAI | ChatAnthropic:\n",
    "    \"\"\"Create LLM instance using configuration.\"\"\"\n",
    "    llm_type_config_keys = _get_llm_type_config_keys()\n",
    "    config_key = llm_type_config_keys.get(llm_type)\n",
    "\n",
    "    if not config_key:\n",
    "        raise ValueError(f\"Unknown LLM type: {llm_type}\")\n",
    "\n",
    "    llm_conf = conf.get(config_key, {})\n",
    "    if not isinstance(llm_conf, dict):\n",
    "        raise ValueError(f\"Invalid LLM configuration for {llm_type}: {llm_conf}\")\n",
    "\n",
    "    # Get configuration from environment variables\n",
    "    env_conf = _get_env_llm_conf(llm_type)\n",
    "\n",
    "    # Merge configurations, with environment variables taking precedence\n",
    "    merged_conf = {**llm_conf, **env_conf}\n",
    "\n",
    "    if not merged_conf:\n",
    "        raise ValueError(f\"No configuration found for LLM type: {llm_type}\")\n",
    "\n",
    "    # if llm_type == \"reasoning\":\n",
    "    #     merged_conf[\"api_base\"] = merged_conf.pop(\"base_url\", None)\n",
    "    model_name = merged_conf.get(\"model\")\n",
    "    if not model_name:\n",
    "        raise ValueError(f\"No model name configured for LLM type: {llm_type}\")\n",
    "\n",
    "    if \"gemini\" in model_name:\n",
    "        return ChatGoogleGenerativeAI(**merged_conf)\n",
    "    elif \"claude\" in model_name:\n",
    "        return ChatAnthropic(**merged_conf)\n",
    "    else:\n",
    "        # Default to ChatOpenAI for OpenAI, Bytedance, and other compatible models.\n",
    "        return ChatOpenAI(**merged_conf)\n",
    "\n",
    "def get_llm_by_type(\n",
    "    llm_type: LLMType,\n",
    ") -> ChatOpenAI | ChatGoogleGenerativeAI | ChatAnthropic:\n",
    "    \"\"\"\n",
    "    Get LLM instance by type. Returns cached instance if available.\n",
    "    \"\"\"\n",
    "    if llm_type in _llm_cache:\n",
    "        return _llm_cache[llm_type]\n",
    "\n",
    "    conf = load_yaml_config(_get_config_file_path())\n",
    "    llm = _create_llm_use_conf(llm_type, conf)\n",
    "    _llm_cache[llm_type] = llm\n",
    "    return llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ff762eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dataclasses import dataclass, fields\n",
    "from typing import Any, Optional\n",
    "\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "\n",
    "\n",
    "\n",
    "@dataclass(kw_only=True)\n",
    "class Configuration:\n",
    "    \"\"\"The configurable fields.\"\"\"\n",
    "\n",
    "\n",
    "    max_plan_iterations: int = 1  # Maximum number of plan iterations\n",
    "    max_step_num: int = 3  # Maximum number of steps in a plan\n",
    "    max_search_results: int = 3  # Maximum number of search results\n",
    "    mcp_settings: dict = None  # MCP settings, including dynamic loaded tools\n",
    "\n",
    "    enable_deep_thinking: bool = False  # Whether to enable deep thinking\n",
    "    number_of_initial_queries: int = 3  # Number of initial search queries to generate in sub-graph\n",
    "\n",
    "    @classmethod\n",
    "    def from_runnable_config(\n",
    "        cls, config: Optional[RunnableConfig] = None\n",
    "    ) -> \"Configuration\":\n",
    "        \"\"\"Create a Configuration instance from a RunnableConfig.\"\"\"\n",
    "        configurable = (\n",
    "            config[\"configurable\"] if config and \"configurable\" in config else {}\n",
    "        )\n",
    "        values: dict[str, Any] = {\n",
    "            f.name: os.environ.get(f.name.upper(), configurable.get(f.name))\n",
    "            for f in fields(cls)\n",
    "            if f.init\n",
    "        }\n",
    "        return cls(**{k: v for k, v in values.items() if v})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3bf24631",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import dataclasses\n",
    "from datetime import datetime\n",
    "from jinja2 import Environment, FileSystemLoader, select_autoescape\n",
    "from langgraph.prebuilt.chat_agent_executor import AgentState\n",
    "\n",
    "# Initialize Jinja2 environment\n",
    "env = Environment(\n",
    "    loader=FileSystemLoader('./prompts'),\n",
    "    autoescape=select_autoescape(),\n",
    "    trim_blocks=True,\n",
    "    lstrip_blocks=True,\n",
    ")\n",
    "def apply_prompt_template(\n",
    "    prompt_name: str, state: AgentState, configurable: Configuration = None\n",
    ") -> list:\n",
    "    \"\"\"\n",
    "    Apply template variables to a prompt template and return formatted messages.\n",
    "\n",
    "    Args:\n",
    "        prompt_name: Name of the prompt template to use\n",
    "        state: Current agent state containing variables to substitute\n",
    "\n",
    "    Returns:\n",
    "        List of messages with the system prompt as the first message\n",
    "    \"\"\"\n",
    "    # Convert state to dict for template rendering\n",
    "    state_vars = {\n",
    "        \"CURRENT_TIME\": datetime.now().strftime(\"%a %b %d %Y %H:%M:%S %z\"),\n",
    "        **state,\n",
    "    }\n",
    "\n",
    "    # Add configurable variables\n",
    "    if configurable:\n",
    "        state_vars.update(dataclasses.asdict(configurable))\n",
    "\n",
    "    try:\n",
    "        template = env.get_template(f\"{prompt_name}.md\")\n",
    "        system_prompt = template.render(**state_vars)\n",
    "        return [{\"role\": \"system\", \"content\": system_prompt}] + state.get(\"messages\", [])\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Error applying template {prompt_name}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ef04e460",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SearchQueryList(BaseModel):\n",
    "    query: List[str] = Field(\n",
    "        description=\"A list of search queries to be used for web research.\"\n",
    "    )\n",
    "    rationale: str = Field(\n",
    "        description=\"A brief explanation of why these queries are relevant to the research topic.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790428f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "from langgraph.types import Command\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def generate_query(state: State, config: RunnableConfig):\n",
    "    \"\"\"LangGraph node that generates a search queries based on the User's question.\n",
    "\n",
    "    Create an optimized search query for web research based on the User's question.\n",
    "\n",
    "    Args:\n",
    "        state: Current graph state containing the User's question\n",
    "        config: Configuration for the runnable, including LLM provider settings\n",
    "\n",
    "    Returns:\n",
    "        Dictionary with state update, including search_query key containing the generated query\n",
    "    \"\"\"\n",
    "    configurable = Configuration.from_runnable_config(config)\n",
    "\n",
    "    # check for custom initial search query count\n",
    "    if state.get(\"initial_search_query_count\") is None:\n",
    "        state[\"initial_search_query_count\"] = configurable.number_of_initial_queries\n",
    "\n",
    "    current_plan = state.get(\"current_plan\")\n",
    "\n",
    "    current_step = None\n",
    "    completed_steps = []\n",
    "    for step in current_plan.steps:\n",
    "        if not step.execution_res:\n",
    "            current_step = step\n",
    "            break\n",
    "        else:\n",
    "            completed_steps.append(step)\n",
    "\n",
    "    if not current_step:\n",
    "        logger.warning(\"No unexecuted step found\")\n",
    "        return Command(goto=\"research_team\")\n",
    "\n",
    "    logger.info(f\"Executing step: {current_step.title}\")\n",
    "\n",
    "    # Format completed steps information\n",
    "    completed_steps_info = \"\"\n",
    "    if completed_steps:\n",
    "        completed_steps_info = \"# Existing Research Findings\\n\\n\"\n",
    "        for i, step in enumerate(completed_steps):\n",
    "            completed_steps_info += f\"## Existing Finding {i + 1}: {step.title}\\n\\n\"\n",
    "            completed_steps_info += f\"<finding>\\n{step.execution_res}\\n</finding>\\n\\n\"\n",
    "\n",
    "\n",
    "\n",
    "    messages = apply_prompt_template(\"generate_query\", state, configurable)\n",
    "    messages.append(\n",
    "        HumanMessage(\n",
    "            content=f\"{completed_steps_info}# Current Task\\n\\n## Title\\n\\n{current_step.title}\\n\\n## Description\\n\\n{current_step.description}\\n\\n## Locale\\n\\n{state.get('locale', 'en-US')}\"\n",
    "        )\n",
    "    )\n",
    "    llm = (\n",
    "        get_llm_by_type(AGENT_LLM_MAP[\"generate_query\"])\n",
    "        .with_structured_output(SearchQueryList)\n",
    "    )\n",
    "\n",
    "    response = llm.invoke(messages)\n",
    "    state[\"query_list\"] = response.query\n",
    "\n",
    "    return {\"query_list\": response.query}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "296bd8c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plan = Plan(\n",
    "    locale=\"en-US\",\n",
    "    has_enough_context=False,\n",
    "    thought=\"To understand the current market trends in AI, we need to gather comprehensive information.\",\n",
    "    title=\"AI Market Research Plan\",\n",
    "    steps=[\n",
    "        Step(\n",
    "            need_search=True,\n",
    "            title=\"Current AI Market Analysis\",\n",
    "            description=\"Collect data on market size, growth rates, major players, and investment trends in AI sector.\",\n",
    "            step_type=StepType.RESEARCH\n",
    "        )\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "48954215",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query_list': ['AI market size and growth rates July 2025', 'Major players in the AI market July 2025', 'Current investment trends in the AI sector July 2025']}\n"
     ]
    }
   ],
   "source": [
    "# generate_query 调用测试\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "\n",
    "# 构造 state\n",
    "state = State(\n",
    "     # Runtime Variables\n",
    "    locale= \"en-US\",\n",
    "    research_topic=  \"\",\n",
    "    model_selection=  None,\n",
    "    observations= [],\n",
    "    plan_iterations=  0,\n",
    "    current_plan= plan,\n",
    "    auto_accepted_plan=  False,\n",
    "\n",
    "    # generate query\n",
    "    initial_search_query_count= 3\n",
    ")\n",
    "\n",
    "# 构造 config\n",
    "config = RunnableConfig({})\n",
    "\n",
    "# 调用 generate_query\n",
    "result = generate_query(state, config)\n",
    "\n",
    "# 打印结果\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a585482",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.types import Send\n",
    "\n",
    "def continue_to_generic_tool_executor(state: QueryGenerationState):\n",
    "    \"\"\"LangGraph node that sends the search queries to the web research node.\n",
    "\n",
    "    This is used to spawn n number of web research nodes, one for each search query.\n",
    "    \"\"\"\n",
    "    return [\n",
    "        Send(\"generic_tool_executor\", {\"search_query\": search_query, \"id\": int(idx)})\n",
    "        for idx, search_query in enumerate(state[\"query_list\"])\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa40b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenericToolState(BaseModel):\n",
    "    \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e504d667",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generic_tool_executor(state: State, config: RunnableConfig\n",
    ") -> GenericToolState:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (.venv-gemini)",
   "language": "python",
   "name": "venv-gemini"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
