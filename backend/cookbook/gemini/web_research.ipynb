{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8617713f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "_ = load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d485a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "from typing import List, Optional\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "class StepType(str, Enum):\n",
    "    RESEARCH = \"research\"\n",
    "    PROCESSING = \"processing\"\n",
    "\n",
    "\n",
    "class Step(BaseModel):\n",
    "    need_search: bool = Field(..., description=\"Must be explicitly set for each step\")\n",
    "    title: str\n",
    "    description: str = Field(..., description=\"Specify exactly what data to collect\")\n",
    "    step_type: StepType = Field(..., description=\"Indicates the nature of the step\")\n",
    "    execution_res: Optional[str] = Field(\n",
    "        default=None, description=\"The Step execution result\"\n",
    "    )\n",
    "\n",
    "class Plan(BaseModel):\n",
    "    locale: str = Field(\n",
    "        ..., description=\"e.g. 'en-US' or 'zh-CN', based on the user's language\"\n",
    "    )\n",
    "    has_enough_context: bool\n",
    "    thought: str\n",
    "    title: str\n",
    "    steps: List[Step] = Field(\n",
    "        default_factory=list,\n",
    "        description=\"Research & Processing steps to get more context\",\n",
    "    )\n",
    "\n",
    "    class Config:\n",
    "        json_schema_extra = {\n",
    "            \"examples\": [\n",
    "                {\n",
    "                    \"has_enough_context\": False,\n",
    "                    \"thought\": (\n",
    "                        \"To understand the current market trends in AI, we need to gather comprehensive information.\"\n",
    "                    ),\n",
    "                    \"title\": \"AI Market Research Plan\",\n",
    "                    \"steps\": [\n",
    "                        {\n",
    "                            \"need_search\": True,\n",
    "                            \"title\": \"Current AI Market Analysis\",\n",
    "                            \"description\": (\n",
    "                                \"Collect data on market size, growth rates, major players, and investment trends in AI sector.\"\n",
    "                            ),\n",
    "                            \"step_type\": \"research\",\n",
    "                        }\n",
    "                    ],\n",
    "                }\n",
    "            ]\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2623192d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import MessagesState\n",
    "\n",
    "class State(MessagesState):\n",
    "    \"\"\"State for the agent system, extends MessagesState with next field.\"\"\"\n",
    "\n",
    "    # Runtime Variables\n",
    "    locale: str = \"en-US\"\n",
    "    research_topic: str = \"\"\n",
    "    model_selection: str = None\n",
    "    observations: list[str] = []\n",
    "    plan_iterations: int = 0\n",
    "    current_plan: Plan | str = None\n",
    "    auto_accepted_plan: bool = False\n",
    "\n",
    "    # generate query\n",
    "    initial_search_query_count: int = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57dae6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "\n",
    "# Define available LLM types\n",
    "LLMType = Literal[\"gemini-2.5-flash\", \"gemini-2.5-pro\", \"claude-3.5-sonnet\", \"gpt-4o-mini\", \"deepseek-chat\", \"doubao-1.5-pro-32k\"]\n",
    "\n",
    "# Define agent-LLM mapping\n",
    "AGENT_LLM_MAP: dict[str, LLMType] = {\n",
    "    \"coordinator\": \"gemini-2.5-flash\",\n",
    "    \"planner\": \"gemini-2.5-flash\",\n",
    "    \"researcher\": \"gpt-4o-mini\",\n",
    "    \"coder\": \"gpt-4o-mini\",\n",
    "    \"reporter\": \"gemini-2.5-flash\",\n",
    "    \"podcast_script_writer\": \"doubao-1.5-pro-32k\",\n",
    "    \"ppt_composer\": \"doubao-1.5-pro-32k\",\n",
    "    \"prose_writer\": \"doubao-1.5-pro-32k\",\n",
    "    \"prompt_enhancer\": \"doubao-1.5-pro-32k\",\n",
    "    \"generate_query\": \"gpt-4o-mini\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb211e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "from typing import Dict, Any\n",
    "\n",
    "def replace_env_vars(value: str) -> str:\n",
    "    \"\"\"Replace environment variables in string values.\"\"\"\n",
    "    if not isinstance(value, str):\n",
    "        return value\n",
    "    if value.startswith(\"$\"):\n",
    "        env_var = value[1:]\n",
    "        return os.getenv(env_var, env_var)\n",
    "    return value\n",
    "\n",
    "def process_dict(config: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"Recursively process dictionary to replace environment variables.\"\"\"\n",
    "    if not config:\n",
    "        return {}\n",
    "    result = {}\n",
    "    for key, value in config.items():\n",
    "        if isinstance(value, dict):\n",
    "            result[key] = process_dict(value)\n",
    "        elif isinstance(value, str):\n",
    "            result[key] = replace_env_vars(value)\n",
    "        else:\n",
    "            result[key] = value\n",
    "    return result\n",
    "\n",
    "_config_cache: Dict[str, Dict[str, Any]] = {}\n",
    "\n",
    "def load_yaml_config(file_path: str) -> Dict[str, Any]:\n",
    "    \"\"\"Load and process YAML configuration file.\"\"\"\n",
    "    # 如果文件不存在，返回{}\n",
    "    if not os.path.exists(file_path):\n",
    "        return {}\n",
    "\n",
    "    # 检查缓存中是否已存在配置\n",
    "    if file_path in _config_cache:\n",
    "        return _config_cache[file_path]\n",
    "\n",
    "    # 如果缓存中不存在，则加载并处理配置\n",
    "    with open(file_path, \"r\") as f:\n",
    "        config = yaml.safe_load(f)\n",
    "    processed_config = process_dict(config)\n",
    "\n",
    "    # 将处理后的配置存入缓存\n",
    "    _config_cache[file_path] = processed_config\n",
    "    return processed_config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b96fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "# Cache for LLM instances\n",
    "_llm_cache: dict[LLMType, ChatOpenAI] = {}\n",
    "\n",
    "def _get_config_file_path() -> str:\n",
    "    \"\"\"Get the path to the configuration file.\"\"\"\n",
    "    return str((Path(os.getcwd()) / \"conf.yaml\").resolve())\n",
    "\n",
    "def _get_llm_type_config_keys() -> dict[str, str]:\n",
    "    \"\"\"Get mapping of LLM types to their configuration keys.\"\"\"\n",
    "    return {\n",
    "        \"gemini-2.5-flash\": \"GEMINI_2_5_FLASH\",\n",
    "        \"gemini-2.5-pro\": \"GEMINI_2_5_PRO\",\n",
    "        \"gpt-4o-mini\": \"GPT_4O_MINI\",\n",
    "        \"doubao-1.5-pro-32k\": \"DOUBAO_1_5_PRO_32K\",\n",
    "        \"claude-3.5-sonnet\": \"CLAUDE_3_5_SONNET\",\n",
    "        \"deepseek-chat\": \"DEEPSEEK_CHAT\",\n",
    "    }\n",
    "\n",
    "def _get_env_llm_conf(llm_type: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Get LLM configuration from environment variables.\n",
    "    Environment variables should follow the format: {LLM_TYPE}__{KEY}\n",
    "    e.g., BASIC_MODEL__api_key, BASIC_MODEL__base_url\n",
    "    \"\"\"\n",
    "    prefix = f\"{llm_type.upper()}_MODEL__\"\n",
    "    conf = {}\n",
    "    for key, value in os.environ.items():\n",
    "        if key.startswith(prefix):\n",
    "            conf_key = key[len(prefix) :].lower()\n",
    "            conf[conf_key] = value\n",
    "    return conf\n",
    "\n",
    "def _create_llm_use_conf(\n",
    "    llm_type: LLMType, conf: Dict[str, Any]\n",
    ") -> ChatOpenAI | ChatGoogleGenerativeAI | ChatAnthropic:\n",
    "    \"\"\"Create LLM instance using configuration.\"\"\"\n",
    "    llm_type_config_keys = _get_llm_type_config_keys()\n",
    "    config_key = llm_type_config_keys.get(llm_type)\n",
    "\n",
    "    if not config_key:\n",
    "        raise ValueError(f\"Unknown LLM type: {llm_type}\")\n",
    "\n",
    "    llm_conf = conf.get(config_key, {})\n",
    "    if not isinstance(llm_conf, dict):\n",
    "        raise ValueError(f\"Invalid LLM configuration for {llm_type}: {llm_conf}\")\n",
    "\n",
    "    # Get configuration from environment variables\n",
    "    env_conf = _get_env_llm_conf(llm_type)\n",
    "\n",
    "    # Merge configurations, with environment variables taking precedence\n",
    "    merged_conf = {**llm_conf, **env_conf}\n",
    "\n",
    "    if not merged_conf:\n",
    "        raise ValueError(f\"No configuration found for LLM type: {llm_type}\")\n",
    "\n",
    "    # if llm_type == \"reasoning\":\n",
    "    #     merged_conf[\"api_base\"] = merged_conf.pop(\"base_url\", None)\n",
    "    model_name = merged_conf.get(\"model\")\n",
    "    if not model_name:\n",
    "        raise ValueError(f\"No model name configured for LLM type: {llm_type}\")\n",
    "\n",
    "    if \"gemini\" in model_name:\n",
    "        return ChatGoogleGenerativeAI(**merged_conf)\n",
    "    elif \"claude\" in model_name:\n",
    "        return ChatAnthropic(**merged_conf)\n",
    "    else:\n",
    "        # Default to ChatOpenAI for OpenAI, Bytedance, and other compatible models.\n",
    "        return ChatOpenAI(**merged_conf)\n",
    "\n",
    "def get_llm_by_type(\n",
    "    llm_type: LLMType,\n",
    ") -> ChatOpenAI | ChatGoogleGenerativeAI | ChatAnthropic:\n",
    "    \"\"\"\n",
    "    Get LLM instance by type. Returns cached instance if available.\n",
    "    \"\"\"\n",
    "    if llm_type in _llm_cache:\n",
    "        return _llm_cache[llm_type]\n",
    "\n",
    "    conf = load_yaml_config(_get_config_file_path())\n",
    "    llm = _create_llm_use_conf(llm_type, conf)\n",
    "    _llm_cache[llm_type] = llm\n",
    "    return llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013e17a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dataclasses import dataclass, fields\n",
    "from typing import Any, Optional\n",
    "\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "\n",
    "\n",
    "\n",
    "@dataclass(kw_only=True)\n",
    "class Configuration:\n",
    "    \"\"\"The configurable fields.\"\"\"\n",
    "\n",
    "\n",
    "    max_plan_iterations: int = 1  # Maximum number of plan iterations\n",
    "    max_step_num: int = 3  # Maximum number of steps in a plan\n",
    "    max_search_results: int = 3  # Maximum number of search results\n",
    "    mcp_settings: dict = None  # MCP settings, including dynamic loaded tools\n",
    "\n",
    "    enable_deep_thinking: bool = False  # Whether to enable deep thinking\n",
    "    number_of_initial_queries: int = 3  # Number of initial search queries to generate in sub-graph\n",
    "\n",
    "    @classmethod\n",
    "    def from_runnable_config(\n",
    "        cls, config: Optional[RunnableConfig] = None\n",
    "    ) -> \"Configuration\":\n",
    "        \"\"\"Create a Configuration instance from a RunnableConfig.\"\"\"\n",
    "        configurable = (\n",
    "            config[\"configurable\"] if config and \"configurable\" in config else {}\n",
    "        )\n",
    "        values: dict[str, Any] = {\n",
    "            f.name: os.environ.get(f.name.upper(), configurable.get(f.name))\n",
    "            for f in fields(cls)\n",
    "            if f.init\n",
    "        }\n",
    "        return cls(**{k: v for k, v in values.items() if v})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013470e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import dataclasses\n",
    "from datetime import datetime\n",
    "from jinja2 import Environment, FileSystemLoader, select_autoescape\n",
    "from langgraph.prebuilt.chat_agent_executor import AgentState\n",
    "\n",
    "# Initialize Jinja2 environment\n",
    "env = Environment(\n",
    "    loader=FileSystemLoader('./prompts'),\n",
    "    autoescape=select_autoescape(),\n",
    "    trim_blocks=True,\n",
    "    lstrip_blocks=True,\n",
    ")\n",
    "def apply_prompt_template(\n",
    "    prompt_name: str, state: AgentState, configurable: Configuration = None\n",
    ") -> list:\n",
    "    \"\"\"\n",
    "    Apply template variables to a prompt template and return formatted messages.\n",
    "\n",
    "    Args:\n",
    "        prompt_name: Name of the prompt template to use\n",
    "        state: Current agent state containing variables to substitute\n",
    "\n",
    "    Returns:\n",
    "        List of messages with the system prompt as the first message\n",
    "    \"\"\"\n",
    "    # Convert state to dict for template rendering\n",
    "    state_vars = {\n",
    "        \"CURRENT_TIME\": datetime.now().strftime(\"%a %b %d %Y %H:%M:%S %z\"),\n",
    "        **state,\n",
    "    }\n",
    "\n",
    "    # Add configurable variables\n",
    "    if configurable:\n",
    "        state_vars.update(dataclasses.asdict(configurable))\n",
    "\n",
    "    try:\n",
    "        template = env.get_template(f\"{prompt_name}.md\")\n",
    "        system_prompt = template.render(**state_vars)\n",
    "        return [{\"role\": \"system\", \"content\": system_prompt}] + state.get(\"messages\", [])\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Error applying template {prompt_name}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3d00a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "# Create agents using configured LLM types\n",
    "def create_agent(agent_name: str, agent_type: str, tools: list, prompt_template: str):\n",
    "    \"\"\"Factory function to create agents with consistent configuration.\"\"\"\n",
    "    return create_react_agent(\n",
    "        name=agent_name,\n",
    "        model=get_llm_by_type(AGENT_LLM_MAP[agent_type]),\n",
    "        tools=tools,\n",
    "        prompt=lambda state: apply_prompt_template(prompt_template, state),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b96e2463",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "from langgraph.types import Command\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "async def _execute_agent_step(\n",
    "    state: State, agent, agent_name: str\n",
    ") -> Command[Literal[\"research_team\"]]:\n",
    "    \"\"\"Helper function to execute a step using the specified agent.\"\"\"\n",
    "    current_plan = state.get(\"current_plan\")\n",
    "    observations = state.get(\"observations\", [])\n",
    "\n",
    "    # Find the first unexecuted step\n",
    "    current_step = None\n",
    "    completed_steps = []\n",
    "    for step in current_plan.steps:\n",
    "        if not step.execution_res:\n",
    "            current_step = step\n",
    "            break\n",
    "        else:\n",
    "            completed_steps.append(step)\n",
    "\n",
    "    if not current_step:\n",
    "        logger.warning(\"No unexecuted step found\")\n",
    "        return Command(goto=\"research_team\")\n",
    "\n",
    "    logger.info(f\"Executing step: {current_step.title}, agent: {agent_name}\")\n",
    "\n",
    "    # Format completed steps information\n",
    "    completed_steps_info = \"\"\n",
    "    if completed_steps:\n",
    "        completed_steps_info = \"# Existing Research Findings\\n\\n\"\n",
    "        for i, step in enumerate(completed_steps):\n",
    "            completed_steps_info += f\"## Existing Finding {i + 1}: {step.title}\\n\\n\"\n",
    "            completed_steps_info += f\"<finding>\\n{step.execution_res}\\n</finding>\\n\\n\"\n",
    "\n",
    "    # Prepare the input for the agent with completed steps info\n",
    "    # TODO: 需要决定是依据research question还是当前step的description来生成input\n",
    "    # 引入research question, 其他交给工具描述去决定用什么参数搜索，同时满足tavily的搜索要求与google的搜索要求\n",
    "    agent_input = {\n",
    "        \"messages\": [\n",
    "            HumanMessage(\n",
    "                content=f\"{completed_steps_info}# Current Task\\n\\n## Title\\n\\n{current_step.title}\\n\\n## Description\\n\\n{current_step.description}\\n\\n## Locale\\n\\n{state.get('locale', 'en-US')}\"\n",
    "            )\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    # Add citation reminder for researcher agent\n",
    "    if agent_name == \"researcher\":\n",
    "        if state.get(\"resources\"):\n",
    "            resources_info = \"**The user mentioned the following resource files:**\\n\\n\"\n",
    "            for resource in state.get(\"resources\"):\n",
    "                resources_info += f\"- {resource.title} ({resource.description})\\n\"\n",
    "\n",
    "            agent_input[\"messages\"].append(\n",
    "                HumanMessage(\n",
    "                    content=resources_info\n",
    "                    + \"\\n\\n\"\n",
    "                    + \"You MUST use the **local_search_tool** to retrieve the information from the resource files.\",\n",
    "                )\n",
    "            )\n",
    "\n",
    "        agent_input[\"messages\"].append(\n",
    "            HumanMessage(\n",
    "                content=\"IMPORTANT: DO NOT include inline citations in the text. Instead, track all sources and include a References section at the end using link reference format. Include an empty line between each citation for better readability. Use this format for each reference:\\n- [Source Title](URL)\\n\\n- [Another Source](URL)\",\n",
    "                name=\"system\",\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # Invoke the agent\n",
    "    default_recursion_limit = 25\n",
    "    try:\n",
    "        env_value_str = os.getenv(\"AGENT_RECURSION_LIMIT\", str(default_recursion_limit))\n",
    "        parsed_limit = int(env_value_str)\n",
    "\n",
    "        if parsed_limit > 0:\n",
    "            recursion_limit = parsed_limit\n",
    "            logger.info(f\"Recursion limit set to: {recursion_limit}\")\n",
    "        else:\n",
    "            logger.warning(\n",
    "                f\"AGENT_RECURSION_LIMIT value '{env_value_str}' (parsed as {parsed_limit}) is not positive. \"\n",
    "                f\"Using default value {default_recursion_limit}.\"\n",
    "            )\n",
    "            recursion_limit = default_recursion_limit\n",
    "    except ValueError:\n",
    "        raw_env_value = os.getenv(\"AGENT_RECURSION_LIMIT\")\n",
    "        logger.warning(\n",
    "            f\"Invalid AGENT_RECURSION_LIMIT value: '{raw_env_value}'. \"\n",
    "            f\"Using default value {default_recursion_limit}.\"\n",
    "        )\n",
    "        recursion_limit = default_recursion_limit\n",
    "\n",
    "    logger.info(f\"Agent input: {agent_input}\")\n",
    "\n",
    "    result = await agent.ainvoke(\n",
    "        input=agent_input, config={\"recursion_limit\": recursion_limit}\n",
    "    )\n",
    "\n",
    "    # Process the result\n",
    "    response_content = result[\"messages\"][-1].content\n",
    "    logger.debug(f\"{agent_name.capitalize()} full response: {response_content}\")\n",
    "\n",
    "    # Update the step with the execution result\n",
    "    current_step.execution_res = response_content\n",
    "    logger.info(f\"Step '{current_step.title}' execution completed by {agent_name}\")\n",
    "\n",
    "    return Command(\n",
    "        update={\n",
    "            \"messages\": [\n",
    "                HumanMessage(\n",
    "                    content=response_content,\n",
    "                    name=agent_name,\n",
    "                )\n",
    "            ],\n",
    "            \"observations\": observations + [response_content],\n",
    "        },\n",
    "        goto=\"research_team\",\n",
    "    )\n",
    "\n",
    "\n",
    "async def _setup_and_execute_agent_step(\n",
    "    state: State,\n",
    "    config: RunnableConfig,\n",
    "    agent_type: str,\n",
    "    default_tools: list,\n",
    ") -> Command[Literal[\"research_team\"]]:\n",
    "    \"\"\"Helper function to set up an agent with appropriate tools and execute a step.\n",
    "\n",
    "    This function handles the common logic for both researcher_node and coder_node:\n",
    "    1. Configures MCP servers and tools based on agent type\n",
    "    2. Creates an agent with the appropriate tools or uses the default agent\n",
    "    3. Executes the agent on the current step\n",
    "\n",
    "    Args:\n",
    "        state: The current state\n",
    "        config: The runnable config\n",
    "        agent_type: The type of agent (\"researcher\" or \"coder\")\n",
    "        default_tools: The default tools to add to the agent\n",
    "\n",
    "    Returns:\n",
    "        Command to update state and go to research_team\n",
    "    \"\"\"\n",
    "    configurable = Configuration.from_runnable_config(config)\n",
    "    mcp_servers = {}\n",
    "    enabled_tools = {}\n",
    "\n",
    "    # Extract MCP server configuration for this agent type\n",
    "    if configurable.mcp_settings:\n",
    "        for server_name, server_config in configurable.mcp_settings[\"servers\"].items():\n",
    "            if (\n",
    "                server_config[\"enabled_tools\"]\n",
    "                and agent_type in server_config[\"add_to_agents\"]\n",
    "            ):\n",
    "                mcp_servers[server_name] = {\n",
    "                    k: v\n",
    "                    for k, v in server_config.items()\n",
    "                    if k in (\"transport\", \"command\", \"args\", \"url\", \"env\")\n",
    "                }\n",
    "                for tool_name in server_config[\"enabled_tools\"]:\n",
    "                    enabled_tools[tool_name] = server_name\n",
    "\n",
    "    # Create and execute agent with MCP tools if available\n",
    "    if mcp_servers:\n",
    "        async with MultiServerMCPClient(mcp_servers) as client:\n",
    "            loaded_tools = default_tools[:]\n",
    "            for tool in client.get_tools():\n",
    "                if tool.name in enabled_tools:\n",
    "                    tool.description = (\n",
    "                        f\"Powered by '{enabled_tools[tool.name]}'.\\n{tool.description}\"\n",
    "                    )\n",
    "                    loaded_tools.append(tool)\n",
    "            agent = create_agent(agent_type, agent_type, loaded_tools, agent_type)\n",
    "            return await _execute_agent_step(state, agent, agent_type)\n",
    "    else:\n",
    "        # Use default tools if no MCP servers are configured\n",
    "        agent = create_agent(agent_type, agent_type, default_tools, agent_type)\n",
    "        return await _execute_agent_step(state, agent, agent_type)\n",
    "\n",
    "\n",
    "async def researcher_node(\n",
    "    state: State, config: RunnableConfig\n",
    ") -> Command[Literal[\"research_team\"]]:\n",
    "    \"\"\"Researcher node that do research\"\"\"\n",
    "    logger.info(\"Researcher node is researching.\")\n",
    "    configurable = Configuration.from_runnable_config(config)\n",
    "    tools = [handoff_to_web_search, crawl_tool]\n",
    "    retriever_tool = get_retriever_tool(state.get(\"resources\", []))\n",
    "    if retriever_tool:\n",
    "        tools.insert(0, retriever_tool)\n",
    "    agent = create_agent()\n",
    "    logger.info(f\"Researcher tools: {tools}\")\n",
    "    return await _execute_agent_step(\n",
    "        state,\n",
    "        config,\n",
    "        \"researcher\",\n",
    "        tools,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c9ea372",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (.venv-gemini)",
   "language": "python",
   "name": "venv-gemini"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
